# Importing the librariesfrom datetime import datetimeimport numpy as npimport matplotlib.pyplot as pltimport pandas as pdfrom pandas.plotting import register_matplotlib_convertersfrom sklearn import preprocessingimport data_loaderimport kerasimport tensorflow as tffrom keras.models import Modelfrom keras.layers import Dense, Dropout, LSTM, Inputfrom keras import optimizersnp.random.seed(10)from tensorflow import set_random_seedset_random_seed(10)register_matplotlib_converters()FIX_HISTORICAL_INTERVAL_DAYS = 60# 1. load the datadef load_data():  #lst_stocks = ['TSLA', 'MSFT', 'GOOGL', 'AMZN', 'NFLX', 'FB']  lst_stock_symbols = ['MSFT']  return data_loader.load_data(lst_stock_symbols)# 2. this would be data viewing and data cleansing#   * look for missing values#   * distribution of data (I need to see the standard deviation as a#     measure of volatility)def describe_data(dct_data):  # get the statistics of the data  for stock_symbol, stock_data in dct_data.items():      print(f'Data for ${stock_symbol}, shape: ${stock_data.shape}:\n')      print(stock_data.head(2))      print('\n')      print(stock_data.info())      print('\n')      print(stock_data.describe())      # stock_data.info()  -> no nulls in the dataset          # 2.1 plot the datadef plot_data(dct_data):  for symbol in dct_data:    plt.plot('date', '1. open', data=dct_data[symbol], label=symbol)    plt.plot('date', '2. high', data=dct_data[symbol], label=symbol)    plt.plot('date', '3. low', data=dct_data[symbol], label=symbol)    plt.plot('date', '4. close', data=dct_data[symbol], label=symbol)    plt.legend()      plt.show()        plt.plot('date', '5. volume', data=dct_data[symbol], label=symbol)    plt.show()            # I want to see the distribution of data on each column    # 3. feature engineeringdef feature_engineering(dct_data, lengthOfHistoricalInterval):  # drop the first column (the date)  dct_data_cleaned = {symbol: data.drop('date', axis=1)                      for symbol, data in dct_data.items()}    # normalize the data  data_normalizer = preprocessing.MinMaxScaler()  dct_data_normalized = {symbol: data_normalizer.fit_transform(data)                        for symbol, data in dct_data_cleaned.items()}      # features available: open, high, low, close, volume  # my intuition is that the open, high, low, volume gives me a measure  # on how desired is a stock and the likelihood that the price will   # go higher or lower.  #  # for LSTM, each example is using the last HistoryPastDays (hyperparameter)   # to predict the follwing day   # and the output value is that of the following day    # my question:   # 1. should I re-order the dataset before droping the date column.   #        # the first symbol in the dictionary is our train model  stock_symbols = list(dct_data_normalized.keys())    # get the stock data  stock_data = dct_data_cleaned[stock_symbols[0]].to_numpy()    # get the normalized new dataset  normalized_stock_data = dct_data_normalized[stock_symbols[0]]    # normalize the dataset  arr_dataset_normalized = np.array([    normalized_stock_data[i : i + lengthOfHistoricalInterval].copy()     for i in range(len(normalized_stock_data) - lengthOfHistoricalInterval)])  # get the output values   # I am interested only in the open value, which is the 1st column.   # I assume that I want to predict the open  arr_output_values_normalized = np.array([    normalized_stock_data[:,0][i + lengthOfHistoricalInterval].copy()    for i in range(len(normalized_stock_data) - lengthOfHistoricalInterval)])  arr_output_values_normalized = np.expand_dims(    arr_output_values_normalized, -1)    # holdon to the unscaled values of the output so that we can plot the results  arr_output_values = np.array([    stock_data[:,0][i + lengthOfHistoricalInterval].copy()     for i in range(len(stock_data) - lengthOfHistoricalInterval)])  arr_output_values = np.expand_dims(arr_output_values, -1)  print(arr_output_values.shape)    print(arr_dataset_normalized.shape)  print(arr_output_values_normalized.shape)   assert(arr_output_values_normalized.shape[0] == arr_output_values.shape[0])    # set a data normalizer for the output  output_normalizer = preprocessing.MinMaxScaler()  output_normalizer.fit(arr_output_values)    return (arr_dataset_normalized,           arr_output_values_normalized,           arr_output_values,          output_normalizer)    # 4. split the dataset into 3 sets#   1. train set#   2. validation set#   3. test setdef split_dataset(dataset, output_values, output_values_unscaled):  n_train_data = int(0.9 * dataset.shape[0])    train_data = dataset[:n_train_data]  train_output = output_values[:n_train_data]    test_data = dataset[n_train_data:]  test_output = output_values[n_train_data:]  test_output_unscaled = output_values_unscaled[n_train_data:]  return (      train_data, train_output,      test_data, test_output, test_output_unscaled    )  def show_results(model, x, y, y_normalizer, title):  preds = y_normalizer.inverse_transform(model.predict(x)).ravel()  gold = y_normalizer.inverse_transform(y).ravel()    df = pd.DataFrame({      'predictions': preds,      'truth': gold,      'MSE': np.mean(np.square(preds-gold))      })  print(f'\n\nResults for model {title}:\n{df}')    # show the linechart  plt.gcf().set_size_inches(22, 15, forward=True)  start = 0  end = -1  plt.plot(gold[start:end], label='real')  plt.plot(preds[start:end], label='predicted')  plt.legend(['Real', 'Predicted'])  plt.title(f'{title}')  plt.savefig(f'redictions_{datetime.now().strftime("%Y%m%d-%H%M%S")}.png')  plt.show()  def execModel(np_stock_train, np_next_day_train,              np_stock_test, np_next_day_test,              np_next_day_normalizer,              dropout_rate, n_epochs=100):    modelHist = {}    # input layer  tf_input = Input(shape=(FIX_HISTORICAL_INTERVAL_DAYS, 5), name='Net_input')    #lstm  lyr_lstm1 = LSTM(150, return_sequences = True, name='Net_lstm1')(tf_input)  lyr_dropout1 = Dropout(rate=dropout_rate, name='Net_lstm1_dropout')(lyr_lstm1)    #lstm  lyr_lstm2 = LSTM(150, name='Net_lstm2')(lyr_dropout1)  lyr_dropout2 = Dropout(rate=dropout_rate, name='Net_lstm2_dropout')(lyr_lstm2)    # dense1  lyr_dense1 = Dense(128, activation='tanh', name='Net_dense1')(lyr_dropout2)    # dense2  tf_output = Dense(1, activation='relu', name='Net_dense2')(lyr_dense1)  regressor = Model(inputs=tf_input, outputs=tf_output,                     name='Regressor_stock_prices')    adam = optimizers.Adam(learning_rate=0.0009)  regressor.compile(optimizer=adam, loss='mse')      # generate an image with the architecture of the model  from keras.utils import plot_model  plot_model(regressor, show_shapes=True)  # train the model and capture the results  hist = regressor.fit(x=np_stock_train, y=np_next_day_train,                       verbose=0,                       epochs=n_epochs, shuffle=True,                        validation_split=0.1)        show_results(regressor, np_stock_test, np_next_day_test,                np_next_day_normalizer,               f'Regressor Stock Prices, dropout:{dropout_rate}, epochs:{n_epochs}')    modelHist['loss']=hist.history['loss']  modelHist['val_loss']=hist.history['val_loss']    print(f'model summary:\n{regressor.summary()}')  print(f'\n\n hist.hostory.keys:\n{hist.history.keys()}')    del regressor  tf.keras.backend.clear_session()  return modelHist# target main thread (for parallelism)if __name__ == '__main__':    # load data and read into it  model_data = load_data()  describe_data(model_data)  # visualize the data  plot_data(model_data)    # feature engineering  dataset, output, output_unscaled, output_normalizer = feature_engineering(    model_data, FIX_HISTORICAL_INTERVAL_DAYS)  # split the dataset  (train_data, train_output,    test_data, test_output, test_output_unscaled) = split_dataset(     dataset, output, output_unscaled)       # build the model  hists = {}  # I. make the experiment with dropout 0.5  modelHist = execModel(train_data, train_output,                       test_data, test_output,                      output_normalizer,                      dropout_rate=0.5,                      n_epochs=150)    hists['train_drop_0.5_loss'] = modelHist['loss']  hists['dev_drop_0.5_loss'] = modelHist['val_loss']    # II. make the experiment with dropout 0.9  # modelHist = execModel(train_data, train_output,   #                     test_data, test_output,  #                     output_normalizer,  #                     dropout_rate=0.9,  #                     n_epochs=150)    # hists['train_drop_0.9_loss'] = modelHist['loss']  # hists['dev_drop_0.9_loss'] = modelHist['val_loss']    # plot all the models  plt.figure(figsize=(20,15))  for hkey in hists:    plt.plot(hists[hkey], label=hkey)  plt.legend(loc="center right")  plt.title("Train vs dev history for all models")  plt.savefig('fig2.png')  plt.show()    